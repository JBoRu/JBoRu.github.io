<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html manifest="welcome.manifest">

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no, minimum-scale=1.0, maximum-scale=1.0">
    <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
    <meta content="yes" name="apple-mobile-web-app-capable">
    <meta content="black" name="apple-mobile-web-app-status-bar-style">
    <meta content="telephone=no" name="format-detection">
    <meta name="author" content="Junyi Li">
    <style type="text/css">
        /* Color scheme stolen from Sergey Karayev */
        
        a {
            color: #07889b;
            /*#1772d0;*/
            text-decoration: none;
        }
        
        a:focus,
        a:hover {
            color: #e37222;
            /*#f7b733;*/
            /*f09228;*/
            text-decoration: none;
        }
        
        body,
        td,
        th,
        tr,
        p,
        a {
            font-family: "TeXGyrePagella", "Palatino Linotype", "Book Antiqua", Palatino, serif;
            /*'Lato', Verdana, Helvetica, sans-serif;*/
            font-size: 15px;
            /*14*/
        }
        
        strong {
            font-family: "TeXGyrePagella", "Palatino Linotype", "Book Antiqua", Palatino, serif;
            /*'Lato', Verdana, Helvetica, sans-serif;*/
            font-size: 15px;
            /*14*/
        }
        
        heading {
            font-family: "TeXGyrePagella", "Palatino Linotype", "Book Antiqua", Palatino, serif;
            /*'Lato', Verdana, Helvetica, sans-serif;*/
            font-size: 22px;
            color: #e37222;
            /*#fc4a1a;*/
        }
        
        heading2 {
            font-family: "TeXGyrePagella", "Palatino Linotype", "Book Antiqua", Palatino, serif;
            /*'Lato', Verdana, Helvetica, sans-serif;*/
            font-size: 18px;
        }
        
        papertitle {
            font-family: "TeXGyrePagella", "Palatino Linotype", "Book Antiqua", Palatino, serif;
            /*'Lato', Verdana, Helvetica, sans-serif;*/
            font-size: 15px;
            /*14*/
            font-weight: 700;
        }
        
        name {
            font-family: "TeXGyrePagella", "Palatino Linotype", "Book Antiqua", Palatino, serif;
            /*'Lato', Verdana, Helvetica, sans-serif;*/
            font-size: 42px;
        }
        
        li:not(:last-child) {
            margin-bottom: 5px;
        }
        
        .one {
            width: 160px;
            height: 160px;
            position: relative;
        }
        
        .two {
            width: 160px;
            height: 160px;
            position: absolute;
            transition: opacity .2s ease-in-out;
            -moz-transition: opacity .2s ease-in-out;
            -webkit-transition: opacity .2s ease-in-out;
        }
        
        .fade {
            transition: opacity .2s ease-in-out;
            -moz-transition: opacity .2s ease-in-out;
            -webkit-transition: opacity .2s ease-in-out;
        }
        
        span.highlight {
            background-color: #ffffd0;
        }
    </style>
    <link href="https://tuchuang-1258543525.cos.ap-beijing.myqcloud.com/20180614_161325021_iOS.jpg" rel="Shortcut Icon" type="image/x-icon">
    <title>Jinhao Jiang (蒋锦昊)</title>

    <link href="./stylesheets/main.css" rel="stylesheet" type="text/css">
    <style id="dark-reader-style" type="text/css">
        @media screen {
            /* Leading rule */
            /*html {
  -webkit-filter: brightness(100%) contrast(100%) grayscale(20%) sepia(10%) !important;
}*/
            /* Text contrast */
            html {
                text-shadow: 0 0 0 !important;
            }
            /* Full screen */
            *:-webkit-full-screen,
            *:-webkit-full-screen * {
                -webkit-filter: none !important;
            }
            /* Page background */
            html {
                background: rgb(255, 255, 255) !important;
            }
        }
    </style>

    <script type="text/javascript">
        function visibility_on(id) {
            var e = document.getElementById(id + "_text");
            if (e.style.display == 'none')
                e.style.display = 'block';
            var e = document.getElementById(id + "_img");
            if (e.style.display == 'none')
                e.style.display = 'block';
        }

        function visibility_off(id) {
            var e = document.getElementById(id + "_text");
            if (e.style.display == 'block')
                e.style.display = 'none';
            var e = document.getElementById(id + "_img");
            if (e.style.display == 'block')
                e.style.display = 'none';
        }

        function toggle_visibility(id) {
            var e = document.getElementById(id + "_text");
            if (e.style.display == 'inline')
                e.style.display = 'block';
            else
                e.style.display = 'inline';
            var e = document.getElementById(id + "_img");
            if (e.style.display == 'inline')
                e.style.display = 'block';
            else
                e.style.display = 'inline';
        }

        function toggle_vis(id) {
            var e = document.getElementById(id);
            if (e.style.display == 'none')
                e.style.display = 'inline';
            else
                e.style.display = 'none';
        }
    </script>

</head>
</div>
<table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tbody>
        <tr>
            <td>
                <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                    <tbody>
                        <tr>
                            <td width="67%" valign="middle">
                                <p align="center">
                                    <name>Jinhao Jiang (蒋锦昊)</name><br>
                                </p>
                                <p style="text-align:justify">
                                    I am a Ph.D. student supervised by <a href="http://playbigdata.ruc.edu.cn/batmanfly/">Prof. Xin Zhao</a> from <a href="http://ai.ruc.edu.cn/english/index.htm">GSAI</a>,
                                    <a href="https://ruc.edu.cn">Renmin University of China</a>. I have a broad interest in natural language processing, with an emphasis on knowledge and reasoning, especially based on pretrained language models (PLMs)
                                    .
                                </p>
                                <p>Email: jiangjinhao at ruc dot edu dot cn</p>
                                <!--         <p>
        <i>There is no authority in science. No one can tell whether your research matters or not. Or how much it matters. All you can do is to contribute to human knowledge and hope it will matter. Even if it doesn't, it does. It eliminates an idea.</i><br>
        </p>
		<p align="right">
		<i>-- Rich Sutton</i>
		</p> -->


                                <p align="center">
                                    <a href="./images/resume.pdf" target="_blank">CV</a> &nbsp;/&nbsp;
                                    <a href="https://scholar.google.com/citations?user=TeFKijMAAAAJ&hl=en" target="_blank">Google Scholar</a> &nbsp;/&nbsp;
                                    <a href="https://github.com/JBoRu" target="_blank"> GitHub </a> &nbsp;/&nbsp;
                                    <a href="https://www.zhihu.com/people/heng-zu-hao" target="_blank">Zhihu</a>
                                </p>
                            </td>
                            <td width="33%">
                                <img src="./images/jiangjinhao.jpg" width="95%">
                            </td>
                        </tr>
                    </tbody>
                </table>

                <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                    <tbody>

                        <tr>
                            <td>
                                <heading>Zhihu Posts</heading>
                                <ul>
                                    <li> <a href="https://zhuanlan.zhihu.com/p/501680110" target="_blank">ACL 2022 主会长文论文分类整理</a></li>
                                    <li> <a href="https://zhuanlan.zhihu.com/p/409061361" target="_blank">可能是目前最全面的知识库复杂问答综述解读</a></li>
                                    <li> <a href="https://zhuanlan.zhihu.com/p/384952066" target="_blank">基于知识图谱的问答系统相关论文分类整理</a></li>

                                    <li> <a href="javascript:toggle_vis('zhihu-blogs')" style="color:black">show more</a> </li>
                                    <div id="zhihu-blogs" style="display:none">
                                        <li> <a href="https://zhuanlan.zhihu.com/p/354274764" target="_blank">GAN理论推导</a></li>
                                    </div>
                                </ul>
                            </td>
                        </tr>

                        <tr>
                            <td width="100%" valign="middle">
                                <heading>Publications</heading> <br><br>

                                <heading2><i>Preprint</i></heading2><br><br>

                                <div onmouseover="document.getElementById('StructGPT').style.display = 'block';" onmouseout="document.getElementById('StructGPT').style.display='none';">
                                    <a href="https://arxiv.org/pdf/2305.09645.pdf">
                                        <font color="#FF0000">!!!!!</font>
                                        <papertitle>StructGPT: A General Framework for Large Language Model to Reason over Structured Data
                                        </papertitle>
                                    </a><br>
                                    <i><strong>Jinhao Jiang</strong></i>&dagger;,
                                    <i>Kun Zhou</i>&dagger;,
                                    <i>Zican Dong</i>
                                    <i>Keming Ye</i>
                                    <i>Wayne Xin Zhao</i>*,
                                    <i>Ji-Rong Wen</i>
                                    <br>
                                    <em>arXiv</em>, 2023 <br>
                                    <a href="https://arxiv.org/pdf/2305.09645.pdf">pdf</a> / <a href="https://github.com/RUCAIBox/StructGPT">code</a>
                                </div>
                                <div id="StructGPT" style="display:none;text-align:justify">
                                    In this paper, we study how to improve the zero-shot reasoning ability of large language models (LLMs) over structured data in a unified way. Inspired by the study on tool augmentation for LLMs, we develop an Iterative Reading-then-Reasoning (IRR) approach
                                    for solving question answering tasks based on structured data, called StructGPT. In our approach, we construct the specialized function to collect relevant evidence from structured data (i.e., reading), and let LLMs
                                    concentrate the reasoning task based on the collected information (i.e., reasoning). Specially, we propose an invoking-linearization-generation procedure to support LLMs in reasoning on the structured data with the
                                    help of the external interfaces. By iterating this procedures with provided interfaces, our approach can gradually approach the target answer to a given query. Extensive experiments conducted on three types of structured
                                    data demonstrate the effectiveness of our approach, which can significantly boost the performance of ChatGPT and achieve comparable performance against the full-data supervised-tuning baselines. Our codes and data are
                                    publicly available at https://github.com/RUCAIBox/StructGPT.
                                </div><br>

                                <div onmouseover="document.getElementById('LLM-Survey').style.display = 'block';" onmouseout="document.getElementById('LLM-Survey').style.display='none';">
                                    <a href="https://arxiv.org/pdf/2303.18223.pdf">
                                        <papertitle>A Survey of Large Language Models
                                        </papertitle>
                                    </a><br>
                                    <i>Wayne Xin Zhao</i>,
                                    <i>Kun Zhou</i>&dagger;,
                                    <i>Junyi Li</i>&dagger;,
                                    <i>Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, <strong>Jinhao Jiang</strong>, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu</i>,
                                    <i>Jian-Yun Nie</i>,
                                    <i>Ji-Rong Wen</i>
                                    <br>
                                    <em>arXiv</em>, 2023 <br>
                                    <a href="https://arxiv.org/pdf/2303.18223.pdf">pdf</a> / <a href="https://github.com/RUCAIBox/LLMSurvey">code</a>
                                </div>
                                <div id="LLM-Survey" style="display:none;text-align:justify">
                                    Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable AI algorithms for comprehending and grasping a language. As a major approach, language modeling
                                    has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been
                                    proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various NLP tasks. Since researchers have found that model scaling can lead to performance improvement, they
                                    further study the scaling effect by increasing the model size to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance
                                    improvement but also show some special abilities that are not present in small-scale language models. To discriminate the difference in parameter scale, the research community has coined the term large language models
                                    (LLM) for the PLMs of significant size. Recently, the research on LLMs has been largely advanced by both academia and industry, and a remarkable progress is the launch of ChatGPT, which has attracted widespread attention
                                    from society. The technical evolution of LLMs has been making an important impact on the entire AI community, which would revolutionize the way how we develop and use AI algorithms. In this survey, we review the recent
                                    advances of LLMs by introducing the background, key findings, and mainstream techniques. In particular, we focus on four major aspects of LLMs, namely pre-training, adaptation tuning, utilization, and capacity evaluation.
                                    Besides, we also summarize the available resources for developing LLMs and discuss the remaining issues for future directions.
                                </div><br>

                                <heading2><i>2023</i></heading2><br><br>

                                <div onmouseover="document.getElementById('StructGPT').style.display = 'block';" onmouseout="document.getElementById('StructGPT').style.display='none';">
                                    <a href="https://arxiv.org/pdf/2305.09645.pdf">
                                        <font color="#FF0000">!!!!!</font>
                                        <papertitle>StructGPT: A General Framework for Large Language Model to Reason over Structured Data
                                        </papertitle>
                                    </a><br>
                                    <i><strong>Jinhao Jiang</strong></i>&dagger;,
                                    <i>Kun Zhou</i>&dagger;,
                                    <i>Zican Dong</i>
                                    <i>Keming Ye</i>
                                    <i>Wayne Xin Zhao</i>*,
                                    <i>Ji-Rong Wen</i>
                                    <br>
                                    <em>EMNLP</em>, 2023 <br>
                                    <a href="https://arxiv.org/pdf/2305.09645.pdf">pdf</a> / <a href="https://github.com/RUCAIBox/StructGPT">code</a>
                                </div>
                                <div id="StructGPT" style="display:none;text-align:justify">
                                    In this paper, we study how to improve the zero-shot reasoning ability of large language models (LLMs) over structured data in a unified way. Inspired by the study on tool augmentation for LLMs, we develop an Iterative Reading-then-Reasoning (IRR) approach
                                    for solving question answering tasks based on structured data, called StructGPT. In our approach, we construct the specialized function to collect relevant evidence from structured data (i.e., reading), and let LLMs
                                    concentrate the reasoning task based on the collected information (i.e., reasoning). Specially, we propose an invoking-linearization-generation procedure to support LLMs in reasoning on the structured data with the
                                    help of the external interfaces. By iterating this procedures with provided interfaces, our approach can gradually approach the target answer to a given query. Extensive experiments conducted on three types of structured
                                    data demonstrate the effectiveness of our approach, which can significantly boost the performance of ChatGPT and achieve comparable performance against the full-data supervised-tuning baselines. Our codes and data are
                                    publicly available at https://github.com/RUCAIBox/StructGPT.
                                </div><br>

                                <div onmouseover="document.getElementById('ReasoningLM').style.display = 'block';" onmouseout="document.getElementById('ReasoningLM').style.display='none';">
                                    <a href="https://arxiv.org/pdf/2305.09645.pdf">
                                        <font color="#FF0000">!!!!!</font>
                                        <papertitle>ReasoningLM: Enabling Structural Subgraph Reasoning in Pre-trained Language Models for Question Answering over Knowledge Graph
                                        </papertitle>
                                    </a><br>
                                    <i><strong>Jinhao Jiang</strong></i>&dagger;,
                                    <i>Kun Zhou</i>&dagger;,
                                    <i>Wayne Xin Zhao</i>*,
                                    <i>Yaliang Li</i>,
                                    <i>Ji-Rong Wen</i>
                                    <br>
                                    <em>EMNLP</em>, 2023 <br>
                                    <a href="https://arxiv.org/pdf/2305.09645.pdf">pdf</a> / <a href="https://github.com/RUCAIBox/ReasoningLM">code</a>
                                </div>
                                <div id="StructGPT" style="display:none;text-align:justify">
                                    Question Answering over Knowledge Graph (KGQA) aims to seek answer entities for the natural language question from a large-scale Knowledge Graph~(KG). To better perform reasoning on KG, recent work typically adopts a pre-trained language model~(PLM) to
                                    model the question, and a graph neural network~(GNN) based module to perform multi-hop reasoning on the KG. Despite the effectiveness, due to the divergence in model architecture, the PLM and GNN are not closely integrated,
                                    limiting the knowledge sharing and fine-grained feature interactions. To solve it, we aim to simplify the above two-module approach, and develop a more capable PLM that can directly support subgraph reasoning for KGQA,
                                    namely ReasoningLM. In our approach, we propose a subgraph-aware self-attention mechanism to imitate the GNN for performing structured reasoning, and also adopt an adaptation tuning strategy to adapt the model parameters
                                    with 20,000 subgraphs with synthesized questions. After adaptation, the PLM can be parameter-efficient fine-tuned on downstream tasks. Experiments show that ReasoningLM surpasses state-of-the-art models by a large margin,
                                    even with fewer updated parameters and less training data. Our codes and data are publicly available at https://github.com/RUCAIBox/StructGPT.
                                </div><br>

                                <div onmouseover="document.getElementById('UniKGQA').style.display = 'block';" onmouseout="document.getElementById('SAFE').style.display='none';">
                                    <a href="https://arxiv.org/abs/2212.00959">
                                        <papertitle>UniKGQA: Unified Retrieval and Reasoning for Solving Multi-hop Question Answering Over Knowledge Graph
                                        </papertitle>
                                    </a><br>
                                    <i><strong>Jinhao Jiang</strong></i>&dagger;,
                                    <i>Kun Zhou</i>&dagger;,
                                    <i>Wayne Xin Zhao</i>*,
                                    <i>Ji-Rong Wen</i>
                                    <br>
                                    <em>International Conference on Learning Representations (ICLR)</em>, 2023<br>
                                    <a href="https://arxiv.org/abs/2212.00959">pdf</a> / <a href="https://github.com/RUCAIBox/UniKGQA">code</a>
                                </div>
                                <div id="UniKGQA" style="display:none;text-align:justify">
                                    Multi-hop Question Answering over Knowledge Graph~(KGQA) aims to find the answer entities that are multiple hops away from the topic entities mentioned in a natural language question on a large-scale Knowledge Graph (KG). To cope with the vast search
                                    space, existing work usually adopts a two-stage approach: it firstly retrieves a relatively small subgraph related to the question and then performs the reasoning on the subgraph to accurately find the answer entities.
                                    Although these two stages are highly related, previous work employs very different technical solutions for developing the retrieval and reasoning models, neglecting their relatedness in task essence. In this paper,
                                    we propose UniKGQA, a novel approach for multi-hop KGQA task, by unifying retrieval and reasoning in both model architecture and parameter learning. For model architecture, UniKGQA consists of a semantic matching module
                                    based on a pre-trained language model~(PLM) for question-relation semantic matching, and a matching information propagation module to propagate the matching information along the edges on KGs. For parameter learning,
                                    we design a shared pre-training task based on question-relation matching for both retrieval and reasoning models, and then propose retrieval- and reasoning-oriented fine-tuning strategies. Compared with previous studies,
                                    our approach is more unified, tightly relating the retrieval and reasoning stages. Extensive experiments on three benchmark datasets have demonstrated the effectiveness of our method on the multi-hop KGQA task. Our
                                    codes and data are publicly available at https://github.com/RUCAIBox/UniKGQA.
                                </div><br>

                                <heading2><i>2022</i></heading2><br><br>

                                <div onmouseover="document.getElementById('SAFE').style.display = 'block';" onmouseout="document.getElementById('SAFE').style.display='none';">
                                    <a href="https://arxiv.org/pdf/2205.01841.pdf">
                                        <papertitle>Great Truths are Always Simple: A Rather Simple Knowledge Encoder for Enhancing the Commonsense Reasoning Capacity of Pre-Trained Models</papertitle>
                                    </a><br>
                                    <i><strong>Jinhao Jiang</strong></i>&dagger;,
                                    <i>Kun Zhou</i>&dagger;,
                                    <i>Wayne Xin Zhao</i>*,
                                    <i>Ji-Rong Wen</i>
                                    <br>
                                    <em>The North American Chapter of the Association for Computational Linguistics (NAACL-Findings)</em>, 2022<br>
                                    <a href="https://arxiv.org/pdf/2205.01841.pdf">pdf</a> / <a href="https://github.com/RUCAIBox/SAFE">code</a>
                                </div>
                                <div id="SAFE" style="display:none;text-align:justify">
                                    Commonsense reasoning in natural language is a desired ability of artificial intelligent systems. For solving complex commonsense reasoning tasks, a typical solution is to enhance pre-trained language models~(PTMs) with a knowledge-aware graph neural
                                    network~(GNN) encoder that models a commonsense knowledge graph~(CSKG). Despite the effectiveness, these approaches are built on heavy architectures, and can't clearly explain how external knowledge resources improve
                                    the reasoning capacity of PTMs. Considering this issue, we conduct a deep empirical analysis, and find that it is indeed relation features from CSKGs (but not node features) that mainly contribute to the performance
                                    improvement of PTMs. Based on this finding, we design a simple MLP-based knowledge encoder that utilizes statistical relation paths as features. Extensive experiments conducted on five benchmarks demonstrate the effectiveness
                                    of our approach, which also largely reduces the parameters for encoding CSKGs. Our codes and data are publicly available at https://github.com/RUCAIBox/SAFE.
                                </div><br>

                                <div onmouseover="document.getElementById('KBQA-Survey-TKDE').style.display = 'block';" onmouseout="document.getElementById('KBQA-Survey-TKDE').style.display='none';">
                                    <a href="https://arxiv.org/pdf/2108.06688.pdf">
                                        <papertitle>Complex Knowledge Base Question Answering: A Survey
                                        </papertitle>
                                    </a><br>
                                    <i>Yunshi Lan</i>&dagger;,
                                    <i>Gaole He</i>&dagger;,
                                    <i><strong>Jinhao Jiang</strong></i>,
                                    <i>Jing Jiang</i>,
                                    <i>Wayne Xin Zhao</i>*,
                                    <i>Ji-Rong Wen</i>
                                    <br>
                                    <em>IEEE Transactions on Knowledge and Data Engineering (TKDE)</em>, 2022 <br>
                                    <a href="https://arxiv.org/pdf/2108.06688.pdf">pdf</a>
                                </div>
                                <div id="KBQA-Survey-TKDE" style="display:none;text-align:justify">
                                    Knowledge base question answering (KBQA) aims to answer a question over a knowledge base (KB). Early studies mainly focused on answering simple questions over KBs and achieved great success. However, their performance on complex questions is still far
                                    from satisfactory. Therefore, in recent years, researchers propose a large number of novel methods, which looked into the challenges of answering complex questions. In this survey, we review recent advances on KBQA
                                    with the focus on solving complex questions, which usually contain multiple subjects, express compound relations, or involve numerical operations. In detail, we begin with introducing the complex KBQA task and relevant
                                    background. Then, we describe benchmark datasets for complex KBQA task and introduce the construction process of these datasets. Next, we present two mainstream categories of methods for complex KBQA, namely semantic
                                    parsing-based (SP-based) methods and information retrieval-based (IR-based) methods. Specifically, we illustrate their procedures with flow designs and discuss their major differences and similarities. After that, we
                                    summarize the challenges that these two categories of methods encounter when answering complex questions, and explicate advanced solutions and techniques used in existing work. Finally, we conclude and discuss several
                                    promising directions related to complex KBQA for future research.
                                </div><br>

                                <heading2><i>2021</i></heading2><br><br>

                                <div onmouseover="document.getElementById('TextBox').style.display = 'block';" onmouseout="document.getElementById('TextBox').style.display='none';">
                                    <a href="https://arxiv.org/pdf/2101.02046.pdf">
                                        <papertitle>TextBox: A Unified, Modularized, and Extensible Framework for Text Generation</papertitle>
                                    </a><br>
                                    <i>Junyi Li</i>&dagger;,
                                    <i>Tianyi Tang</i>&dagger;,
                                    <i>Gaole He</i>,
                                    <i><strong>Jinhao Jiang</strong></i>,
                                    <i>Xiaoxuan Hu</i>,
                                    <i>Puzhao Xie</i>,
                                    <i>Zhipeng Chen</i>,
                                    <i>Zhuohao Yu</i>,
                                    <i>Wayne Xin Zhao</i>*,
                                    <i>Ji-Rong Wen</i>
                                    <br>
                                    <em>The 59th Annual Meeting of the Association for Computational Linguistics (ACL)</em>, 2021, System Demonstration <br>
                                    <a href="https://arxiv.org/pdf/2101.02046.pdf">pdf</a> / <a href="https://github.com/RUCAIBox/TextBox">code</a>
                                </div>
                                <div id="TextBox" style="display:none;text-align:justify">
                                    We release an open library, called TextBox, which provides a unified, modularized, and extensible text generation framework. TextBox aims to support a broad set of text generation tasks and models. In TextBox, we implements several text generation models
                                    on benchmark datasets, covering the categories of VAE, GAN, pre-trained language models, etc. Meanwhile, our library maintains sufficient modularity and extensibility by properly decomposing the model architecture,
                                    inference, learning process into highly reusable modules, which allows easily incorporating new models into our framework. It is specially suitable for researchers and practitioners to efficiently reproduce baseline
                                    models and develop new models. TextBox is implemented based on PyTorch, and released under Apache License 2.0 at the link https://github.com/RUCAIBox/TextBox.
                                </div><br>

                                <div onmouseover="document.getElementById('KBQA-Survey-IJCAI').style.display = 'block';" onmouseout="document.getElementById('KBQA-Survey-IJCAI').style.display='none';">
                                    <a href="https://arxiv.org/pdf/2105.11644.pdf">
                                        <papertitle>A survey on complex knowledge base question answering: Methods, challenges and solutions</papertitle>
                                    </a><br>
                                    <i>Yunshi Lan</i>&dagger;,
                                    <i>Gaole He</i>&dagger;,
                                    <i><strong>Jinhao Jiang</strong></i>,
                                    <i>Jing Jiang</i>,
                                    <i>Wayne Xin Zhao</i>*,
                                    <i>Ji-Rong Wen</i>
                                    <br>
                                    <em>The 30th International Joint Conference on Artificial Intelligence (IJCAI)</em>, 2021, Survey Track <br>
                                    <a href="https://arxiv.org/pdf/2105.11644.pdf">pdf</a>
                                </div>
                                <div id="KBQA-Survey-IJCAI" style="display:none;text-align:justify">
                                    Knowledge base question answering (KBQA) aims to answer a question over a knowledge base (KB). Recently, a large number of studies focus on semantically or syntactically complicated questions. In this paper, we elaborately summarize the typical challenges
                                    and solutions for complex KBQA. We begin with introducing the background about the KBQA task. Next, we present the two mainstream categories of methods for complex KBQA, namely semantic parsing-based (SP-based) methods
                                    and information retrieval-based (IR-based) methods. We then review the advanced methods comprehensively from the perspective of the two categories. Specifically, we explicate their solutions to the typical challenges.
                                    Finally, we conclude and discuss some promising directions for future research.
                                </div><br> * Corresponding author <br> &dagger; Equal contribution
                            </td>
                        </tr>

                        <!-- <tr>
                            <td width="100%" valign="middle">
                                <heading>Open Source Projects</heading> <br><br> (Most of my research work are open-source. Here are some my preferable projects!)
                                <ul>
                                    <li><a href="https://github.com/RUCAIBox/TextBox" target="_black">TextBox</a><br>A unified, comprehensive and efficient framework for reproducing and developing text generation algorithms, covering more than 20 base models
                                        and nearly 10 benchmarks.</li>
                                </ul>
                            </td>
                        </tr> -->

                        <!-- <tr>
                            <td width="100%" valign="middle">
                                <heading>Professional Services</heading> <br><br>
                                <ul>
                                    <li>Reviewer
                                        <ul>
                                            <li>Journal: TALLIP</li>
                                            <li>Conference: AAAI 2021-22, IJCAI 2021-22, KDD 2021</li>
                                        </ul>
                                    </li>
                                    <li>Chair
                                        <ul>
                                            <li>CSSNLP 2020 (Co-Chair)</li>
                                        </ul>
                                    </li>
                                </ul>
                            </td>
                        </tr> -->

                        <tr>
                            <td width="100%" valign="middle">
                                <heading>Selected Awards and Honors</heading> <br><br>
                                <ul>
                                    <li>2021 Outstanding Graduates of Sichuan Province (winning ratio 3.7%), Education Department of Sichuan.</li>
                                    <li>2020 China National Scholarship (top 1.5%), Ministry of Education of the People's Republic of China.</li>
                                    <li>2019 China National Scholarship (top 1.5%), Ministry of Education of the People's Republic of China.</li>
                                    <li>2019 Meritorious Winner (winning ratio 7.09%) in Mathematical Contest In Modeling, the COMAP of American.</li>
                                </ul>
                            </td>
                        </tr>

                        <tr>
                            <td width="100%" valign="middle">
                                <heading>Education</heading> <br><br>
                                <ul>
                                    <li>Ph.D. student of Artificial Intelligence, Renmin University of China & Universite de Montreal, 2021-present
                                    </li>
                                    <li>B.Sc. of Computer Science, University of Electronic Science and Technology of China, 2017-2021
                                    </li>
                                </ul>
                            </td>
                        </tr>

                        <tr>
                            <td width="100%" valign="middle">
                                <center>
                                    <script type="text/javascript" src="//rf.revolvermaps.com/0/0/1.js?i=571ds4qroi6&amp;s=220&amp;m=7&amp;v=true&amp;r=false&amp;b=000000&amp;n=false&amp;c=ff0000" async="async"></script>
                                </center>
                            </td>
                        </tr>

                    </tbody>
                </table>


                <script>
                    (function(i, s, o, g, r, a, m) {
                        i['GoogleAnalyticsObject'] = r;
                        i[r] = i[r] || function() {
                            (i[r].q = i[r].q || []).push(arguments)
                        }, i[r].l = 1 * new Date();
                        a = s.createElement(o),
                            m = s.getElementsByTagName(o)[0];
                        a.async = 1;
                        a.src = g;
                        m.parentNode.insertBefore(a, m)
                    })(window, document, 'script', 'https://www.google-analytics.com/analytics.js', 'ga');

                    ga('create', 'UA-59618557-1', 'auto');
                    ga('send', 'pageview');
                </script>

            </td>
        </tr>
    </tbody>
</table>

<!--footer start-->
<footer class="footer">
    <p>Copyright 2021. All Rights Reserved by Jinhao Jiang.</p>
</footer>
<!--footer end-->

</body>

</html>